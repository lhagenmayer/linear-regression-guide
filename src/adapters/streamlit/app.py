"""
Streamlit-Anwendung - 100% Plattform-Agnostisch.

Nutzt dieselbe API-Schicht wie externe Frontends (Next.js, Vite, etc.).
Dies stellt eine konsistente Logik Ã¼ber alle BenutzeroberflÃ¤chen sicher.

Architektur:
    Streamlit App â†’ API Layer â†’ Core Pipeline
    
    Identisch zu:
    Next.js App â†’ HTTP â†’ API Layer â†’ Core Pipeline
"""

import streamlit as st
import numpy as np
from typing import Dict, Any, Optional

from ...config import get_logger
from ...api import RegressionAPI, ContentAPI, AIInterpretationAPI
from ...core.domain.value_objects import SplitConfig

logger = get_logger(__name__)


def run_streamlit_app():
    """Haupt-Einstiegspunkt fÃ¼r die Streamlit-Anwendung."""
    # Seiten-Konfiguration
    st.set_page_config(
        page_title="Regression & Klassifikation",
        page_icon="ğŸ§ ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Custom CSS fÃ¼r modernes UI/UX Design
    from .styles import inject_custom_css, render_hero
    inject_custom_css()
    
    # Initalisierung der APIs
    regression_api = RegressionAPI()
    content_api = ContentAPI()
    ai_api = AIInterpretationAPI()
    
    # Sidebar (Seitenleiste)
    with st.sidebar:
        st.markdown("""
        <div style="margin-bottom: 2rem;">
            <div class="api-badge">API-GESTÃœTZT</div>
            <h1 style="font-size: 1.5rem; margin: 0;">RegAnalysis</h1>
            <p style="color: #94a3b8; font-size: 0.9rem;">Interaktive Lernplattform</p>
        </div>
        """, unsafe_allow_html=True)
        
        analysis_type = st.radio(
            "Analysetyp",
            ["Einfache Regression", "Multiple Regression", "BinÃ¤re Klassifikation"],
            key="analysis_type",
            label_visibility="collapsed"
        )
        
        st.markdown("---")
        
        # Datensatz-Auswahl
        datasets_response = regression_api.get_datasets()
        st.markdown("### ğŸ“Š Datensatz")
        
        # Standard-Parameter
        method = "logistic"
        k_neighbors = 3
        
        if analysis_type == "Einfache Regression":
            dataset_options = {d["name"]: d["id"] for d in datasets_response["data"]["simple"]}
            dataset_name = st.selectbox("WÃ¤hle Datensatz:", list(dataset_options.keys()), key="dataset_simple")
            dataset_id = dataset_options[dataset_name]
            n_points = st.slider("StichprobengrÃ¶ÃŸe", 20, 200, 50, key="n_simple")
            
        elif analysis_type == "Multiple Regression":
            dataset_options = {d["name"]: d["id"] for d in datasets_response["data"]["multiple"]}
            dataset_name = st.selectbox("WÃ¤hle Datensatz:", list(dataset_options.keys()), key="dataset_multiple")
            dataset_id = dataset_options[dataset_name]
            n_points = st.slider("StichprobengrÃ¶ÃŸe", 30, 200, 75, key="n_multiple")
            
        else: # BinÃ¤re Klassifikation
            # Auswahl an DatensÃ¤tzen fÃ¼r die Klassifikation
            cls_options = {
                "ğŸ FrÃ¼chte (2D)": "fruits",
                "ğŸ”¢ Ziffern (64D)": "digits",
                "ğŸ“± Elektronik (BinÃ¤r-Verschneidung)": "binary_electronics",
                "ğŸ  Immobilien (BinÃ¤r-Verschneidung)": "binary_housing",
                "ğŸ¥ WHO Health (Extern)": "who_health",
                "ğŸ¦ Weltbank (Extern)": "world_bank",
            }
            dataset_name = st.selectbox("WÃ¤hle Datensatz:", list(cls_options.keys()), key="dataset_cls")
            dataset_id = cls_options[dataset_name]
            n_points = st.slider("StichprobengrÃ¶ÃŸe", 50, 500, 100, step=10, key="n_cls")
            
            st.markdown("### ğŸ§  Modell")
            method_display = st.selectbox("Methode", ["Logistische Regression", "K-Nearest Neighbors"], key="method_select")
            method = "logistic" if "Logistische" in method_display else "knn"
            
            if method == "knn":
                k_neighbors = st.slider("Nachbarn (k)", 1, 25, 3, key="k_knn")
                
            # Konfiguration des Data-Splits (Nur bei Klassifikation relevant)
            with st.expander("Data Split & Stratification", expanded=True):
                st.markdown("Konfiguriere das VerhÃ¤ltnis von Trainings- zu Testdaten.")
                
                col1, col2 = st.columns(2)
                with col1:
                    train_size = st.slider(
                        "Trainings-Anteil", 
                        min_value=0.1, 
                        max_value=0.9, 
                        value=0.8, 
                        step=0.05,
                        key="train_size_slider"
                    )
                with col2:
                    stratify = st.checkbox(
                        "Stratifizierung", 
                        value=False,
                        key="stratify_checkbox",
                        help="HÃ¤lt die KlassenverhÃ¤ltnisse in beiden Splits gleich."
                    )
                    
                # Live-Vorschau der Klassenverteilung via API
                try:
                    preview_noise_val = 0.2
                    preview_seed_val = 42
                    
                    preview = content_api.get_split_preview(
                        dataset=dataset_id, 
                        train_size=train_size,
                        stratify=stratify, 
                        seed=preview_seed_val,
                        n=n_points,
                        noise=preview_noise_val
                    )
                    
                    if preview["success"]:
                        stats = preview["stats"]
                        import pandas as pd
                        import plotly.express as px
                        
                        dist_data = []
                        for k, v in stats["train_distribution"].items():
                            dist_data.append({"Klasse": str(k), "Anzahl": v, "Set": "Train"})
                        for k, v in stats["test_distribution"].items():
                            dist_data.append({"Klasse": str(k), "Anzahl": v, "Set": "Test"})
                            
                        df_dist = pd.DataFrame(dist_data)
                        fig_dist = px.bar(
                            df_dist, x="Set", y="Anzahl", color="Klasse", barmode="group",
                            height=150, title=None
                        )
                        fig_dist.update_layout(margin=dict(l=0, r=0, t=0, b=0), showlegend=False)
                        st.plotly_chart(fig_dist, use_container_width=True)
                except Exception:
                    pass
        
        st.markdown("### âš™ï¸ Parameter")
        noise = st.slider("Rausch-Niveau", 0.0, 2.0, 0.2 if analysis_type == "BinÃ¤re Klassifikation" else 0.4, 0.1, key="noise")
        seed = st.number_input("Zufalls-Seed", 1, 9999, 42, key="seed")
        
        st.markdown("---")
        
        # API Statusanzeige
        status = ai_api.get_status()
        if status["status"]["configured"]:
            st.success("âœ… KI verbunden")
        else:
            st.warning("âš ï¸ KI Fallback")
            
    # Hauptinhalt
    if "hero_shown" not in st.session_state:
        st.session_state.hero_shown = True
        
    # Tabs fÃ¼r Analyse vs. Datenexplorer
    tab_analysis, tab_data = st.tabs(["ğŸ“Š Analyse", "ğŸ—ƒï¸ Daten-Explorer"])
    
    with tab_analysis:
        if analysis_type == "Einfache Regression":
            render_hero("Einfache Regression", "Erkunde ZusammenhÃ¤nge, analysiere Residuen und meistere statistische Modellierung.")
            render_simple_regression(content_api, ai_api, dataset_id, n_points, noise, seed)
        elif analysis_type == "Multiple Regression":
            render_hero("Multiple Regression", "Multivariate Analyse mit interaktiven 3D-Visualisierungen.")
            render_multiple_regression(content_api, ai_api, dataset_id, n_points, noise, seed)
        else:
            render_hero("Machine Learning", "Von der logistischen Regression bis hin zur KNN-Klassifikation.")
            render_classification(content_api, ai_api, dataset_id, n_points, noise, seed, method, k_neighbors, train_size, stratify)

    with tab_data:
        st.markdown(f"### ğŸ—ƒï¸ Rohdaten: {dataset_name}")
        st.markdown(f"**ID:** `{dataset_id}` | **Samples:** {n_points}")
        
        try:
             # Rohdaten via API laden
             raw_resp = content_api.get_dataset_raw(dataset_id)
             if raw_resp.get("success"):
                 data = raw_resp["data"]["data"]
                 columns = raw_resp["data"]["columns"]
                 
                 import pandas as pd
                 df = pd.DataFrame(data)
                 # Spalten sortieren, Target nach hinten
                 if "Target" in columns and "Target" in df.columns:
                      cols = [c for c in df.columns if c != "Target"] + ["Target"]
                      df = df[cols]
                 
                 st.dataframe(df, use_container_width=True)
                 
                 csv = df.to_csv(index=False).encode('utf-8')
                 st.download_button(
                     "ğŸ“¥ CSV Herunterladen",
                     csv,
                     f"{dataset_id}.csv",
                     "text/csv",
                     key='download-csv'
                 )
             else:
                 st.error(f"Daten konnten nicht geladen werden: {raw_resp.get('error')}")
        except Exception as e:
            st.error(f"Fehler im Daten-Explorer: {e}")


def render_simple_regression(
    content_api: ContentAPI,
    ai_api: AIInterpretationAPI,
    dataset: str,
    n_points: int,
    noise: float,
    seed: int
):
    """Rendert die einfache Regressionsanalyse unter Nutzung der API."""
    
    # API-Aufruf (identisch zu einem externen Frontend)
    with st.spinner("ğŸ“Š Lade Daten via API..."):
        response = content_api.get_simple_content(
            dataset=dataset,
            n=n_points,
            noise=noise,
            seed=seed
        )
    
    if not response["success"]:
        st.error(f"API Fehler: {response.get('error', 'Unbekannter Fehler')}")
        return
    
    # Daten aus dem API-Response extrahieren
    content = response["content"]
    plots = response["plots"]
    stats = response["stats"]
    data = response["data"]
    
    # Statistiken fÃ¼r den Renderer aufbereiten
    stats_dict = _flatten_stats(stats, data)
    
    # Inhalte mittels StreamlitContentRenderer visualisieren
    from ..renderers import StreamlitContentRenderer
    
    renderer = StreamlitContentRenderer(
        plots={},  # Interaktive Plots werden on-demand generiert
        data={
            "x": np.array(data["x"]),
            "y": np.array(data["y"]),
            "x_label": data["x_label"],
            "y_label": data["y_label"],
        },
        stats=stats_dict
    )
    
    # Inhaltsstruktur aus der API rekonstruieren
    from ...content import SimpleRegressionContent
    content_builder = SimpleRegressionContent(stats_dict, {})
    content_obj = content_builder.build()
    
    # Rendern des edukativen Inhalts
    renderer.render(content_obj)
    
    # KI-Interpretation am Ende hinzufÃ¼gen
    _render_ai_interpretation(ai_api, stats_dict)


def render_multiple_regression(
    content_api: ContentAPI,
    ai_api: AIInterpretationAPI,
    dataset: str,
    n_points: int,
    noise: float,
    seed: int
):
    """Rendert die multiple Regressionsanalyse."""
    
    # API-Aufruf
    with st.spinner("ğŸ“Š Lade Daten via API..."):
        response = content_api.get_multiple_content(
            dataset=dataset,
            n=n_points,
            noise=noise,
            seed=seed
        )
    
    if not response["success"]:
        st.error(f"API Fehler: {response.get('error', 'Unbekannter Fehler')}")
        return
    
    # Daten extrahieren
    content = response["content"]
    plots = response["plots"]
    stats = response["stats"]
    data = response["data"]
    
    # Statistiken flach klopfen
    stats_dict = _flatten_multiple_stats(stats, data)
    
    # Rendering
    from ..renderers import StreamlitContentRenderer
    from ...content import MultipleRegressionContent
    
    renderer = StreamlitContentRenderer(
        plots={},
        data={
            "x1": np.array(data["x1"]),
            "x2": np.array(data["x2"]),
            "y": np.array(data["y"]),
            "x1_label": data["x1_label"],
            "x2_label": data["x2_label"],
            "y_label": data["y_label"],
        },
        stats=stats_dict
    )
    
    content_builder = MultipleRegressionContent(stats_dict, {})
    content_obj = content_builder.build()
    
    renderer.render(content_obj)
    
    # KI-Interpretation
    _render_ai_interpretation(ai_api, stats_dict)


def _render_ai_interpretation(ai_api: AIInterpretationAPI, stats_dict: Dict[str, Any]):
    """Rendert die KI-gestÃ¼tzte Interpretation der Statistiken."""
    
    st.subheader("ğŸ¤– KI-Interpretation des R-Outputs")
    st.markdown("""
    <div style="background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%); 
                padding: 1.5rem; border-radius: 1rem; color: white; margin: 1rem 0;">
        <p style="margin: 0; opacity: 0.9;">
            Lass dir alle statistischen Werte gesamtheitlich von Perplexity AI erklÃ¤ren.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Statusabfrage Ã¼ber die API
    status = ai_api.get_status()
    if status["status"]["configured"]:
        st.success("âœ… Perplexity API verbunden")
    else:
        st.warning("âš ï¸ Kein API-Key - Fallback-Interpretation wird verwendet")
        st.caption("Setze `PERPLEXITY_API_KEY` als Umgebungsvariable")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("""
        Klicke auf **"R-Output interpretieren"** fÃ¼r eine vollstÃ¤ndige ErklÃ¤rung:
        - Zusammenfassung des Modells
        - Interpretation der Koeffizienten
        - Bewertung der ModellgÃ¼te
        - Signifikanz-ErklÃ¤rung
        - Praktische Bedeutung
        """)
    
    with col2:
        interpret_clicked = st.button(
            "ğŸ” Interpretieren",
            type="primary",
            use_container_width=True,
            key="ai_interpret_btn"
        )
    
    # R-Output anzeigen (Summary Statistik)
    with st.expander("ğŸ“„ R-Output anzeigen"):
        r_output_response = ai_api.get_r_output(stats_dict)
        if r_output_response["success"]:
            st.code(r_output_response["r_output"], language="r")
    
    # Management des Session States fÃ¼r die API-Antwort
    if "ai_interpretation_result" not in st.session_state:
        st.session_state.ai_interpretation_result = None
    
    if interpret_clicked:
        with st.spinner("ğŸ¤– AI analysiert via API..."):
            # API-Aufruf (identisch zum HTMX-Vorgehen im Flask-Frontend)
            response = ai_api.interpret(stats=stats_dict, use_cache=True)
            st.session_state.ai_interpretation_result = response
    
    # Anzeige der Interpretation
    if st.session_state.ai_interpretation_result:
        response = st.session_state.ai_interpretation_result
        
        st.markdown("### ğŸ“Š Interpretation")
        
        # Hauptinhalt (Markdown von der KI)
        interpretation = response.get("interpretation", {})
        st.markdown(interpretation.get("content", "Keine Interpretation verfÃ¼gbar."))
        
        # Metadaten zur Anfrage
        if response.get("success"):
            meta_cols = st.columns(4)
            meta_cols[0].caption(f"ğŸ“¡ {interpretation.get('model', 'N/A')}")
            meta_cols[1].caption(f"â±ï¸ {interpretation.get('latency_ms', 0):.0f}ms")
            
            usage = response.get("usage", {})
            if usage:
                meta_cols[2].caption(f"ğŸ“ {usage.get('total_tokens', 'N/A')} Tokens")
            
            meta_cols[3].caption(f"ğŸ’¾ {'Cached' if interpretation.get('cached') else 'Live'}")
        
        # Quellenangaben/Zitate
        citations = response.get("citations", [])
        if citations:
            with st.expander("ğŸ“š Quellen"):
                for i, citation in enumerate(citations, 1):
                    st.markdown(f"{i}. [{citation}]({citation})")


    renderer.render(content_obj)
    
    # AI Interpretation
    _render_ai_interpretation(ai_api, stats_dict)


def render_classification(
    content_api: ContentAPI,
    ai_api: AIInterpretationAPI,
    dataset: str,
    n_points: int,
    noise: float,
    seed: int,
    method: str,
    k_neighbors: int,
    train_size: float,
    stratify: bool
):
    """Rendert die Klassifikationsanalyse (Machine Learning)."""
    
    with st.spinner(f"ğŸ§  Trainiere {method.upper()} Modell via API..."):
        response = content_api.get_classification_content(
            dataset=dataset,
            n=n_points,
            noise=noise,
            seed=seed,
            method=method,
            k=k_neighbors,
            train_size=train_size,
            stratify=stratify
        )
        
    if not response["success"]:
        st.error(f"ML API Fehler: {response.get('error')}")
        return
        
    # Extraktion der Daten
    content_dict = response["content"]
    plots_dict = response["plots"]
    stats_dict = response["stats"]
    data_dict = response["data"]
    results_dict = response.get("results", {})
    test_metrics = results_dict.get("test_metrics")
    
    # Anzeige der Performance-Metriken (Train vs Test)
    st.markdown("### ğŸ“‰ Modell-Performance")
    m_col1, m_col2, m_col3, m_col4 = st.columns(4)
    
    metrics = stats_dict # Trainings-Metriken
    
    with m_col1:
        st.metric("Genauigkeit (Train)", f"{metrics.get('accuracy',0):.2%}")
        if test_metrics:
            st.metric("Genauigkeit (Test)", f"{test_metrics.get('accuracy',0):.2%}", 
                     delta=f"{test_metrics.get('accuracy',0) - metrics.get('accuracy',0):.2%}")
            
    with m_col2:
        st.metric("PrÃ¤zision (Train)", f"{metrics.get('precision',0):.2f}")
        if test_metrics:
            st.metric("PrÃ¤zision (Test)", f"{test_metrics.get('precision',0):.2f}")

    with m_col3:
        st.metric("Recall (Train)", f"{metrics.get('recall',0):.2f}")
        if test_metrics:
            st.metric("Recall (Test)", f"{test_metrics.get('recall',0):.2f}")
            
    with m_col4:
        st.metric("F1 Score (Train)", f"{metrics.get('f1',0):.2f}")
        if test_metrics:
            st.metric("F1 Score (Test)", f"{test_metrics.get('f1',0):.2f}")
            
    st.markdown("---")
    
    # Rekonstruktion des Inhalts-Objekts
    from ...infrastructure.content.structure import EducationalContent
    try:
        content_obj = EducationalContent.from_dict(content_dict)
    except Exception as e:
        st.error(f"Fehler beim Laden des Inhalts: {e}")
        return
    
    # Rekonstruktion der Plots (Umwandlung von JSON in Plotly-Objekte)
    import plotly.graph_objects as go
    renderer_plots = {}
    
    if plots_dict:
        # Standard-Plots aus der PlotCollection
        for key in ["scatter", "residuals", "diagnostics"]:
           if plots_dict.get(key):
               renderer_plots[key] = go.Figure(plots_dict[key])
        
        # ZusÃ¤tzliche Plots
        if plots_dict.get("extra"):
            for k, v in plots_dict["extra"].items():
                if v:
                    renderer_plots[k] = go.Figure(v)
                
    # Initialisierung des Renderers
    from ..renderers import StreamlitContentRenderer
    
    # Vorbereitung der Daten fÃ¼r interaktive Plots
    renderer_data = {
        "x": np.array(data_dict.get("X", [])),
        "y": np.array(data_dict.get("y", [])),
        "target_names": data_dict.get("target_names", []),
        "feature_names": stats_dict.get("feature_names", [])
    }
    
    renderer = StreamlitContentRenderer(
        plots=renderer_plots,
        data=renderer_data,
        stats=stats_dict
    )
    
    # Rendern des Inhalts und der KI-Interpretation
    renderer.render(content_obj)
    _render_ai_interpretation(ai_api, stats_dict)


if __name__ == "__main__":
    run_streamlit_app()
